---
title: "BR8"
author: "Steph Jordan"
output: html_notebook
---

```{r}
library(rstan)
library(bayesplot)
library(janitor)
library(bayesrules)
library(tidyverse)
```


## Exercise 8.1

1. develop the prior model
2. collect data
3. develop the posterior model by combining the data and the prior model

## Exercise 8.2
 a. The main drawback is that this is only one estimate of the posterior lambda; different posterior models could have yielded a range of different lambdas. What is more useful is to present the lambda value and the confidence interval associated with it. 
 
 b. There's a 95% chance that lambda will fall between 1 and 3.4. 
 
## Exercise 8.3

a. Yes: >40% of dogs have a license (hypothesis), <40% of dogs have a license (null hypothesis)
b. No--no specific proposal to test
c. Yes: >60% of voters support a new regulation, <60% of voters support the regulation
d. No--"about" 3 is not precise enough (we need greater than, less than, or equal to in order to formulate a hypothesis and null hypothesis)

## Exercise 8.4

a. Posterior odds is the "odds" form of posterior probability. It is the probability that a hypothesis is true given observation of a certain outcome divided by the probability that a hypothesis is false given observation of a certain outcome. In other words, its the likelihood, given the fact that a certain condition is true, of the hypothesis being true. 

b. Prior odds is simply the probability that a certain condition is true divided by the probability that it is false. There is no prior, previous condition here. 

c. Bayes factor is the ratio of posterior odds to prior odds. It demonstrates how much our understanding of the hypothesis changed given our observed data. 

## Exercise 8.5

a. Sampling variability in the data (the mean observed depends on the exact random sample we get) and posterior variability in pi (our confidence interval illustrates our confidence in the mean posterior probability). The former refers to deviation in all the samples we could draw from our population; the latter refers to the fact that our posterior estimate for the average is still just an estimate--it is not *the* average. Therefore, we must consider all possible values of pi, even though some are more likely than others. 

b. How many of the incoming students will major in computer science. 

c. It is conditional on both the data and the parameter. 

## Exercise 8.6

a. We'll use qbeta()
```{r}
qbeta(c(0.025, 0.975), 4, 5)
```

b. We'll use qbeta()
```{r}
qbeta(c(0.2, 0.8), 4, 5)
```

c. We'll use qgamma()
```{r}
qgamma(c(0.025, 0.975), 1, 8)
```

## Exercise 8.7 

a. We'll use qgamma()

```{r}
qgamma(c(0.005, 0.995), 1, 5)
```

b. We'll use qnorm()

```{r}
qnorm(c(0.025, 0.975), 10, 2)
```

c. We'll use qnorm()

```{r}
qnorm(c(0.005, 0.995), -3, 1)
```

## Exercise 8.8

a. We will first plot the distribution to see visually where the top 95% might be.
```{r}
plot_gamma(1, 5)
```
We know from this plot that the bottom of the confidence interval will be percentile 0 (lambda==0), and we can use our quantile function to figure out where 0+95=95th percentile is. 

```{r}
qgamma(c(0, 0.95), 1, 5)
```
The confidence interval is (0, 0.599)

b. Using the middle 95 approach, we get:
```{r}
qgamma(c(0.025, 0.975), 1, 5)
```

c. They are not the same. The middle 95% reaches a significantly higher upper bound than the highest posterior density credible interval. The highest posterior density credible interval is more applicable here, because the graph is significiantly skewed, so it produces a more accurate estimation of the range of values of pi than the middle 95% estimation. 

d. We'll start by plotting the distribution
```{r}
plot_normal(-13, 2)
```

Since the majority of values are distributed symmetrically around the mean/mode (-13), we know the highest 95% interval will be equivalent to the middle 95%. Therefore, we can use our known quantile calculations to determine the highest 95% interval.

```{r}
qnorm(c(0.025, 0.0975), -13, 2)
```

e. Using the middle 95% approach yields the same result:
```{r}
qnorm(c(0.025, 0.0975), -13, 2)
```

f. They are the same, since the values are distributed symmetrically around the mean. Therefore, the middle 95% will be equivalent to the highest 95%. 

## Exercise 8.9

a. We'll use the method demonstrated in the textbook, and subtract 1 since we are testing for pi>0.4
```{r}
post_prob <- 1- pbeta(0.40, 4, 3)
post_prob
```

b. We'll again use the formula for posterior odds given in the book
```{r}
post_odds <- post_prob/(1-post_prob)
post_odds
```

c. Same formula as posterior odds but with prior probabilities

```{r}
prior_prob <- 1- pbeta(0.4, 1, 0.8)
prior_odds <- prior_prob/(1-prior_prob)
prior_odds
```

d. Bayes factor is the result of dividing the prior and posterior:
```{r}
post_odds/prior_odds
```
Since BF>1, the plausibility of the alternative hypothesis being true increased given the data. 

e. The alternative hypothesis (pi>0.4) is more likely than the null hypothesis (pi<=0.4), and the greater probability of the alternative strengthens in light of the observed data. 

## Exercise 8.10

a. We'll use the method demonstrated in the textbook
```{r}
post_prob <- pnorm(0.52, 5, 3)
post_prob
```

b. We'll again use the formula for posterior odds given in the book
```{r}
post_odds <- post_prob/(1-post_prob)
post_odds
```

c. Same formula as posterior odds but with prior probabilities

```{r}
prior_prob <- pnorm(0.52, 10, 10)
prior_odds <- prior_prob/(1-prior_prob)
prior_odds
```

d. Bayes factor is the result of dividing the prior and posterior:
```{r}
post_odds/prior_odds
```
Since BF<1, the plausibility of the alternative hypothesis being true decreased given the data. 

e. The alternative hypothesis (pi<0.52) is less likely than the null hypothesis (pi>=0.52), and the data strengthened the fact that the alternative hypothesis is less likely than the null hypothesis. 

## Exercise 8.11 , 8.12, 8.13 
require calculus?

## Exercise 8.14

a. Beta-Binomial, because pi will represent a fraction between 0 and 1 of the percentage of adults who don't believe in climate change. 

b. Beta(1, 4): I think that most adults in the U.S. now believe in climate change. I would guess 1/4 (25%) of adults do not. 

c. The authors are much more pessimistic.

d. Loading data
```{r}
data(pulse_of_the_nation)
```

```{r}
glimpse(pulse_of_the_nation)
climate <- pulse_of_the_nation |> select(climate_change)
climate
```

```{r}
clim_count <- climate |> summarise(x=length(climate_change))
clim_count

not_real <- climate |> filter(climate_change=="Not Real At All") |> summarise(x=length(climate_change))
not_real
```
Calculate the sample proportion
```{r}
not_real/clim_count
```
e. Our posterior model of pi is as follows: 
$$ Beta(151, 852)$$
A middle 95% CI is as follows:
```{r}
qbeta(c(0.025, 0.975), 151, 852)
```


## Exercise 8.15

a. It is more likely that pi is greater than 0.1 than pi is less than 0.1, since the entire 95% middle confidence interval is above 0.1.

b. Calculating posterior probability of the alternative hypothesis:
```{r}
post_prob <- 1- pbeta(0.1, 151, 852)
post_prob
```

It is highly likely (99%) that pi is greater than 0.1.

c. Calculating the Bayes factor
```{r}
post_odds <- post_prob/(1-post_prob)

prior_prob <- pbeta(0.1, 1, 2)
prior_odds <- prior_prob/(1-prior_prob)

post_odds/prior_odds
```

Since the Bayes factor is VERY much greater than 1, the plausibility of the alternative hypothesis being true greatly increased with the data.

d. Pi is likely to be greater than 0.1.

## Exercise 8.16

a. Building the MCMC model
```{r}
# STEP 1: DEFINE the model
bb_model <- "
  data {
    int<lower = 0, upper = 1000> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(1000, pi);
    pi ~ beta(1, 2);
  }
"
```

```{r}
# STEP 2: SIMULATE the posterior
bb_sim <- stan(model_code = bb_model, data = list(Y = 150), 
               chains = 4, iter = 5000*2, seed = 84735)
```
Illustrate the traces
```{r}
mcmc_trace(bb_sim, pars = "pi", size = 0.1)
```

## Exercise 8.17 
a. Approximating 95% confidence interval. First, we'll plot the data
```{r}
plot_beta(alpha = 151, beta = 852) + lims(x = c(0.1,0.2))

# MCMC posterior approximation
mcmc_dens(bb_sim, pars = "pi")
```

Now, we'll get the interval from the MCMC approximation

```{r}
bb_chains_df <- as.data.frame(bb_sim, pars = "lp__", include = FALSE)
# Calculate posterior summaries of pi
bb_chains_df %>% 
  summarize(post_mean = mean(pi), 
            post_median = median(pi),
            post_mode = sample_mode(pi),
            lower_95 = quantile(pi, 0.025),
            upper_95 = quantile(pi, 0.975))
```

Now we'll find the same information using the tidy shortcut
```{r}
library(broom.mixed)
tidy(bb_sim, conf.int = TRUE, conf.level = 0.95)
```

b. We'll use the method outlined in the book:
```{r}
bb_chains_df %>% 
  mutate(exceeds = pi > 0.10) %>% 
  tabyl(exceeds)
```

According to the MCMC model, all approximated values are greater than 0.1 (so there is a 100% chance that pi>0.1).

c. The lower bound of the 95% confidence interval is very close to our estimates from 8.14 (0.12 to 0.17).


## Exercise 8.18
a. We'll again follow the book's suggestions:
```{r}
set.seed(1)

# Predict a value of Y' for each pi value in the chain
bb_chains_df <- bb_chains_df %>% 
  mutate(y_predict = rbinom(length(pi), size = 100, prob = pi))

# Check it out
bb_chains_df %>% 
  head(3)

# Plot the 20,000 predictions
ggplot(bb_chains_df, aes(x = y_predict)) + 
  stat_count()
```
b. The number of adults who don't believe in climate change, after adding 100 more into our sample pool, is most likely to range between 12 and 18.

c. Finding probability that at least 20 don't believe in climate change
NOTE: check methodology here

```{r}
denom <- bb_chains_df  %>% 
  summarize(total=length(pi))

nom <- bb_chains_df %>% 
  filter(y_predict>=20) %>% 
  summarize(total=length(pi))

nom/denom

```

## Exercise 8.19
a. Normal-Normal, because measurments are likely to be symmetrical (like height)

b. As shown below, Normal(200, 30) yields a majority of values between 140 and 260. 

```{r}
plot_normal(200, 30)
```


c. Calculating stats from penguins package
```{r}
data("penguins_bayes")
glimpse(penguins_bayes)
pengs <- penguins_bayes |> filter(species=="Adelie")
pengs <- pengs |> drop_na()
pengs |>  summarize(mean=mean(flipper_length_mm), num=length(flipper_length_mm), sd=sd(flipper_length_mm))
```
d. Posterior model
```{r}
summarize_normal_normal(mean=200, sd=5.48, sigma=42.5, y_bar=190.1, n=146)
```


Calculating 95% confidence interval:
```{r}
qnorm(c(0.025, 0.975), 192.99, 2.96^(2))
```


## Exercise 8.20

## Exercise 8.21

## Exercise 8.22

## Exercise 8.23

## Exercise 8.24





